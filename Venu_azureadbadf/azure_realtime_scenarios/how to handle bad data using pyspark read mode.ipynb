{"cells":[{"cell_type":"markdown","source":["## Working with Corrupt Data\n* ETL pipelines need robust solutions to handle corrupt data. \n* This is because data corruption scales as the size of data and complexity of the data application grow. Corrupt data includes:\n\n\n* __`Missing information`__\n* __`Incomplete information`__\n* __`Schema mismatch`__\n* __`Differing formats or data types`__\n* __`User errors when writing data producers`__\n\n* Since ETL pipelines are built to be automated, production-oriented solutions must ensure pipelines behave as expected. This means that data engineers must both expect and systematically handle corrupt records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82aa9da9-1e5d-4002-9ca2-5340062deb90"}}},{"cell_type":"markdown","source":["## How to handle Bad Records in spark and those types?\n\n* There are tree types of modes available while reading and creating dataframe.\n* Dealing with bad Records Verify correctness of the data When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema.\n* __`PERMISSIVE (default)`__: nulls are inserted for fields that could not be parsed correctly\n* __`DROPMALFORMED`__: drops lines that contain fields that could not be parsed\n* __`FAILFAST`__: aborts the reading if any malformed data is found"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b0d345b-c153-41e9-b835-1df665417df3"}}},{"cell_type":"code","source":["dbutils.fs.put(\"/FileStore/tables/channels.csv\",\"\"\"CHANNEL_ID,CHANNEL_DESC,CHANNEL_CLASS,CHANNEL_CLASS_ID,CHANNEL_TOTAL,CHANNEL_TOTAL_ID\n3,Direct Sales,Direct,12,Channel total,1\n9,Tele Sales,Direct,12,Channel total,1\n5,Catalog,Indirect,13,Channel total,1\n4,Internet,Indirect,13,Channel total,1\n2,Partners,Others,14,Channel total,1\n12,Partners,Others,14,Channel total,1,45,ram,3434\nsample,Partners,Others,14,Channel total,1,45,ram,3434\n10 Partners Others 14 Channel total 1\n11 Partners Others 14 Channel total 1\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"028f266d-be53-4640-940b-893ff29ee87e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Wrote 459 bytes.\nOut[1]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Wrote 459 bytes.\nOut[1]: True"]}}],"execution_count":0},{"cell_type":"code","source":["help(spark.read.csv)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"675900e5-a3ad-4239-817f-4154c524a031"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Help on method csv in module pyspark.sql.readwriter:\n\ncsv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n    \n    This function will go through the input once to determine the input schema if\n    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n    \n    .. versionadded:: 2.0.0\n    \n    Parameters\n    ----------\n    path : str or list\n        string, or list of strings, for input path(s),\n        or RDD of Strings storing CSV rows.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n        in the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n    >>> df.dtypes\n    [('_c0', 'string'), ('_c1', 'string')]\n    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n    >>> df2 = spark.read.csv(rdd)\n    >>> df2.dtypes\n    [('_c0', 'string'), ('_c1', 'string')]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Help on method csv in module pyspark.sql.readwriter:\n\ncsv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n    \n    This function will go through the input once to determine the input schema if\n    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n    \n    .. versionadded:: 2.0.0\n    \n    Parameters\n    ----------\n    path : str or list\n        string, or list of strings, for input path(s),\n        or RDD of Strings storing CSV rows.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n        in the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n    >>> df.dtypes\n    [('_c0', 'string'), ('_c1', 'string')]\n    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n    >>> df2 = spark.read.csv(rdd)\n    >>> df2.dtypes\n    [('_c0', 'string'), ('_c1', 'string')]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema_channels = StructType([StructField('CHANNEL_ID', IntegerType(), True), \n                              StructField('CHANNEL_DESC', StringType(), True), \n                              StructField('CHANNEL_CLASS', StringType(), True), \n                              StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n                              StructField('CHANNEL_TOTAL', StringType(), True), \n                              StructField('CHANNEL_TOTAL_ID', IntegerType(), True),\n                              StructField('BadData', StringType(), True)])\ndf_channels = spark.read.schema(schema_channels).csv(\"/FileStore/tables/channels.csv\",header=True,columnNameOfCorruptRecord=\"BadData\")\ndisplay(df_channels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c07ac212-7aff-4550-bcd5-40b68e9d961a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[3,"Direct Sales","Direct",12,"Channel total",1,null],[9,"Tele Sales","Direct",12,"Channel total",1,null],[5,"Catalog","Indirect",13,"Channel total",1,null],[4,"Internet","Indirect",13,"Channel total",1,null],[2,"Partners","Others",14,"Channel total",1,null],[12,"Partners","Others",14,"Channel total",1,"12,Partners,Others,14,Channel total,1,45,ram,3434"],[null,"Partners","Others",14,"Channel total",1,"sample,Partners,Others,14,Channel total,1,45,ram,3434"],[null,null,null,null,null,null,"10 Partners Others 14 Channel total 1"],[null,null,null,null,null,null,"11 Partners Others 14 Channel total 1"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"CHANNEL_ID","type":"\"integer\"","metadata":"{}"},{"name":"CHANNEL_DESC","type":"\"string\"","metadata":"{}"},{"name":"CHANNEL_CLASS","type":"\"string\"","metadata":"{}"},{"name":"CHANNEL_CLASS_ID","type":"\"integer\"","metadata":"{}"},{"name":"CHANNEL_TOTAL","type":"\"string\"","metadata":"{}"},{"name":"CHANNEL_TOTAL_ID","type":"\"integer\"","metadata":"{}"},{"name":"BadData","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CHANNEL_ID</th><th>CHANNEL_DESC</th><th>CHANNEL_CLASS</th><th>CHANNEL_CLASS_ID</th><th>CHANNEL_TOTAL</th><th>CHANNEL_TOTAL_ID</th><th>BadData</th></tr></thead><tbody><tr><td>3</td><td>Direct Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>9</td><td>Tele Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>5</td><td>Catalog</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>4</td><td>Internet</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>2</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>12</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>12,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>null</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>sample,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>10 Partners Others 14 Channel total 1</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11 Partners Others 14 Channel total 1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_channels.cache()\ngood_data = df_channels.filter(\"BadData is null\").drop(\"BadData\")\nbad_data = df_channels.filter(\"BadData is not null\").select(\"BadData\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28370b06-7e1b-48ff-a0d8-de6f181bf7da"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(bad_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd35baeb-9c4f-48de-9239-2817d19fdd77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["12,Partners,Others,14,Channel total,1,45,ram,3434"],["sample,Partners,Others,14,Channel total,1,45,ram,3434"],["10 Partners Others 14 Channel total 1"],["11 Partners Others 14 Channel total 1"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"BadData","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>BadData</th></tr></thead><tbody><tr><td>12,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>sample,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>10 Partners Others 14 Channel total 1</td></tr><tr><td>11 Partners Others 14 Channel total 1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### `Handling bad records and files`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83ed52a2-6b3e-4bf9-bd8f-f5c349d448c0"}}},{"cell_type":"markdown","source":["* if any invalid data based schema. it will create timestamp based folder and place json log file\n* it will store three fields,filename with path,error reason,errored data \n* Azure Databricks provides a unified interface for handling bad records and files without interrupting Spark jobs. \n* You can obtain the exception records/files and reasons from the exception logs by setting the data source option `badRecordsPath`. \n* `badRecordsPath` specifies a path to store exception files for recording the information about bad records for `CSV and JSON` sources and bad files for all the file-based built-in sources (for example, Parquet)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c5f07f3-5e72-4b13-8f04-565519cdbb13"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema_channels = StructType([StructField('CHANNEL_ID', IntegerType(), True), \n                              StructField('CHANNEL_DESC', StringType(), True), \n                              StructField('CHANNEL_CLASS', StringType(), True), \n                              StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n                              StructField('CHANNEL_TOTAL', StringType(), True), \n                              StructField('CHANNEL_TOTAL_ID', IntegerType(), True)])\ndf_channels = spark.read.schema(schema_channels).option(\"badRecordsPath\",\"/channels/badata/\").csv(\"/FileStore/tables/channels.csv\",header=True,columnNameOfCorruptRecord=\"BadData\")\ndisplay(df_channels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a73302f4-77e2-4358-a497-340f4cc3e5c5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_baddata = spark.read.json(\"/channels/badata/*/*/\")\ndisplay(df_baddata)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73a10975-646a-4cb0-861b-2c204b7a18ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/FileStore/tables/channels.csv","org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record","12,Partners,Others,14,Channel total,1,45,ram,3434"],["dbfs:/FileStore/tables/channels.csv","org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record","sample,Partners,Others,14,Channel total,1,45,ram,3434"],["dbfs:/FileStore/tables/channels.csv","org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record","10 Partners Others 14 Channel total 1"],["dbfs:/FileStore/tables/channels.csv","org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record","11 Partners Others 14 Channel total 1"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"reason","type":"\"string\"","metadata":"{}"},{"name":"record","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>reason</th><th>record</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record</td><td>12,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record</td><td>sample,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record</td><td>10 Partners Others 14 Channel total 1</td></tr><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record</td><td>11 Partners Others 14 Channel total 1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs  head dbfs:/channels/badata/20220928T164614/bad_records/part-00000-946a209a-465a-46a3-acd3-e33adf9fd287"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d686903-5685-427b-b896-947d1e7e3eb0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;12,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;sample,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;10 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;11 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;12,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;sample,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;10 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;11 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record&quot;}\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(df_channels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fdcfa9c-bdf1-4451-9a46-a76c9f7e1070"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["3","Direct Sales","Direct",12,"Channel total",1],["9","Tele Sales","Direct",12,"Channel total",1],["5","Catalog","Indirect",13,"Channel total",1],["4","Internet","Indirect",13,"Channel total",1],["2","Partners","Others",14,"Channel total",1],["12","Partners","Others",14,"Channel total",1],["sample","Partners","Others",14,"Channel total",1],["10 Partners Others 14 Channel total 1",null,null,null,null,null],["11 Partners Others 14 Channel total 1",null,null,null,null,null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"CHANNEL_ID","type":"\"string\"","metadata":"{}"},{"name":"CHANNEL_DESC","type":"\"string\"","metadata":"{}"},{"name":"CHANNEL_CLASS","type":"\"string\"","metadata":"{}"},{"name":"CHANNEL_CLASS_ID","type":"\"integer\"","metadata":"{}"},{"name":"CHANNEL_TOTAL","type":"\"string\"","metadata":"{}"},{"name":"CHANNEL_TOTAL_ID","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CHANNEL_ID</th><th>CHANNEL_DESC</th><th>CHANNEL_CLASS</th><th>CHANNEL_CLASS_ID</th><th>CHANNEL_TOTAL</th><th>CHANNEL_TOTAL_ID</th></tr></thead><tbody><tr><td>3</td><td>Direct Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>9</td><td>Tele Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>5</td><td>Catalog</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>4</td><td>Internet</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>2</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td></tr><tr><td>12</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td></tr><tr><td>sample</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td></tr><tr><td>10 Partners Others 14 Channel total 1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>11 Partners Others 14 Channel total 1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5442fc8-183e-42e5-9a17-62dbf1d58995"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"how to handle bad data using pyspark read mode","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":367429991000884}},"nbformat":4,"nbformat_minor":0}
